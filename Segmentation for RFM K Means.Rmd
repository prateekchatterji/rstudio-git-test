---
title: "Segementation for RFM using K Means_v1"
output:
  html_document:
    df_print: paged
---

---
title: "Segmentation Using K Means"
output: html_notebook
---


Data Source: [archive.ics.uci.edu/](https://archive.ics.uci.edu/ml/datasets/Online+Retail)

Customer Segmentation is a subject of intense analysis in the corridors of Business Planning. There are many ways in which Anlaysts approach segmenting customers; however, the basis lies in the Business Problem at hand.

Dataset -- In this particluar case, I have borrowed data from UCI archive by Prof Draquin Chen as published in Journal of Database Marketing and Customer Strategy Management. 

Approach -- THe data is cleaned,preprocessed and subject to K-means clustering.

Caveat -- It is assumed that customers of the dataset are indeed different from each other. Practiaclly, such cases are rare. 


```{r library, warning=FALSE, message=FALSE, echo = FALSE}
options(java.parameters = "-Xmx1024m")

library(tidyverse)
library(XLConnect)
library(lubridate)
library(RCurl)

library(ggplot2)
library(scales)

library(plyr)

library(NbClust)


library(car)
library(rgl)

```

### Import Data

```{r Importing Data , warning=FALSE, message=FALSE, echo = FALSE}
# Import data
# https://archive.ics.uci.edu/ml/datasets/Online+Retail
library(XLConnect)

# raw.data <- readxl::read_xlsx("Online Retail.xlsx", sheet=1)
# takes too long

library(readr)
# raw.data <- read_csv("Online Retail.csv", 
#     col_types = cols(InvoiceDate = col_datetime(format = "%m/%d/%Y %H:%M")))

library(RCurl)
githubURL <- "https://raw.github.com/prateekchatterji/rstudio-git-test/master/RFM.RData"
download.file(githubURL,"rawdata")
load("rawdata")

# Prepare data
str(raw.data)
data <- raw.data # the data takes a couple of minutes to import, so I keep raw.data as a backup
str(data)
```

### Cleaning Data
Missing Data removal: ID numbers are unavailable for a number of data rows. Remove any observations with missing ID numbers. Analysis would be conducted at customer level. 

```{r Cleaning data , warning=FALSE, message=FALSE, echo = FALSE}
length(unique(data$CustomerID))
sum(is.na(data$CustomerID))
data <- subset(data, !is.na(data$CustomerID))
```

### Tidying Data
Limiting date range to one full year.

```{r Tidying Data, warning=FALSE, message=FALSE, echo = FALSE}
range(data$InvoiceDate, na.rm = TRUE)
data <- subset(data, InvoiceDate >= "2010-12-09")
range(data$InvoiceDate)

```

### Adding Geographic Constraints
Using on United Kingdom's data share

```{r Adding Geographic Constraints, warning=FALSE, message=FALSE, echo = FALSE}
table(data$Country)
data <- subset(data, Country == "United Kingdom")
```


### Complete Dataset

```{r Complete Dataset, warning=FALSE, message=FALSE, echo = FALSE}
length(unique(data$InvoiceNo))
rRow <- length(unique(data$CustomerID))
```


We now have a dataset of 19,140 unique invoices and 3,891 unique customers.




### Identify returns
Invoice purchases are different from Invoice of Returns. Adjust accordingly.

```{r Identify Returns , warning=FALSE, message=FALSE, echo = FALSE}
data$item.return <- grepl("C", data$InvoiceNo, fixed=TRUE) 
data$purchase.invoice <- ifelse(data$item.return=="TRUE", 0, 1)

```

## RFM Variables
Create Customer level Dataset

```{r Create customer level dataset, warning=FALSE, message=FALSE, echo = FALSE}
customers <- as.data.frame(unique(data$CustomerID))
names(customers) <- "CustomerID"

```


Create Recency Dataset

```{r Recenc, warning=FALSE, message=FALSE, echo = FALSE}
data$recency <- as.Date("2011-12-10") - as.Date(data$InvoiceDate)

# remove returns so only consider the data of most recent *purchase*
temp <- subset(data, purchase.invoice == 1)

# Obtain # of days since most recent purchase
recency <- aggregate(recency ~ CustomerID, data=temp, FUN=min, na.rm=TRUE)
remove(temp)

# Add recency to customer data
customers <- merge(customers, recency, by="CustomerID", all=TRUE, sort=TRUE) 
remove(recency)

customers$recency <- as.numeric(customers$recency)

```


Create Frequency Dataset

```{r Frequency, warning=FALSE, message=FALSE, echo = FALSE}
customer.invoices <- subset(data, select = c("CustomerID","InvoiceNo", "purchase.invoice"))
customer.invoices <- customer.invoices[!duplicated(customer.invoices), ]
customer.invoices <- customer.invoices[order(customer.invoices$CustomerID),]
row.names(customer.invoices) <- NULL

# Number of invoices/year (purchases only)
annual.invoices <- aggregate(purchase.invoice ~ CustomerID, data=customer.invoices, FUN=sum, na.rm=TRUE)
names(annual.invoices)[names(annual.invoices)=="purchase.invoice"] <- "frequency"

# Add # of purchase invoices to customers data
customers <- merge(customers, annual.invoices, by="CustomerID", all=TRUE, sort=TRUE) 
remove(customer.invoices, annual.invoices)

range(customers$frequency)
table(customers$frequency)

# Remove customers who have not made any purchases in the past year
customers <- subset(customers, frequency > 0) 
```



Create Monetary Value Dataset

```{r Monetary Value, warning=FALSE, message=FALSE, echo = FALSE}
# Total spent on each item on an invoice
data$Amount <- data$Quantity * data$UnitPrice

# Aggregated total sales to customer
annual.sales <- aggregate(Amount ~ CustomerID, data=data, FUN=sum, na.rm=TRUE)
names(annual.sales)[names(annual.sales)=="Amount"] <- "monetary"

# Add monetary value to customers dataset
customers <- merge(customers, annual.sales, by="CustomerID", all.x=TRUE, sort=TRUE)
remove(annual.sales)

# Identify customers with negative monetary value numbers, as they were presumably returning purchases from the preceding year
hist(customers$monetary)
customers$monetary <- ifelse(customers$monetary < 0, 0, customers$monetary) # reset negative numbers to zero
hist(customers$monetary)
```

## Pareto Principle

80/20 Rule

In this context, it implies that ~80% of sales would be produced by the top ~20% of customers. These 20% represent the high-value, important customers a business would want to protect.

To make a point about outliers below, I create some simple segments here by looking at the top customers who produced 80% of annual sales for the year. In this dataset, 80% of the annual sales are produced by the top 29% of customers, so the percentage isn't quite 20%, but it's not that far off and it does illustrate that there's a smallish segment producing the bulk of the value.


```{r Pareto, warning=FALSE, message=FALSE, echo = FALSE}
customers <- customers[order(-customers$monetary),] #Remove Customers who have not made any purchases in the past year

# Apply Pareto Principle (80/20 Rule)
pareto.cutoff <- 0.8 * sum(customers$monetary)
customers$pareto <- ifelse(cumsum(customers$monetary) <= pareto.cutoff, "Top 20%", "Bottom 80%")
customers$pareto <- factor(customers$pareto, levels=c("Top 20%", "Bottom 80%"), ordered=TRUE)
levels(customers$pareto)

round(prop.table(table(customers$pareto)), 2)

remove(pareto.cutoff)

customers <- customers[order(customers$CustomerID),]
```


## Preprocess data 

k-means clustering requires continuous variables and works best with relatively normally-distributed, standardized input variables. Standardizing the input variables is quite important; otherwise, input variables with larger variances will have commensurately greater influence on the results. Below, I transform our three input variables to reduce positive skew and then standardize them as z-scores.


```{r Preprocess Data, warning=FALSE, message=FALSE, echo = FALSE}
# Log-transform positively-skewed variables
customers$recency.log <- log(customers$recency)
customers$frequency.log <- log(customers$frequency)
customers$monetary.log <- customers$monetary + 0.1 # can't take log(0), so add a small value to remove zeros
customers$monetary.log <- log(customers$monetary.log)

# Z-scores
customers$recency.z <- scale(customers$recency.log, center=TRUE, scale=TRUE)
customers$frequency.z <- scale(customers$frequency.log, center=TRUE, scale=TRUE)
customers$monetary.z <- scale(customers$monetary.log, center=TRUE, scale=TRUE)
```




Visualize data 

```{r Visualize data, warning=FALSE, message=FALSE, echo = FALSE}


library(ggplot2)
library(scales)

# Original scale
scatter.1 <- ggplot(customers, aes(x = frequency, y = monetary))
scatter.1 <- scatter.1 + geom_point(aes(colour = recency, shape = pareto))
scatter.1 <- scatter.1 + scale_shape_manual(name = "80/20 Designation", values=c(17, 16))
scatter.1 <- scatter.1 + scale_colour_gradient(name="Recency\n(Days since Last Purchase))")
scatter.1 <- scatter.1 + scale_y_continuous(label=dollar)
scatter.1 <- scatter.1 + xlab("Frequency (Number of Purchases)")
scatter.1 <- scatter.1 + ylab("Monetary Value of Customer (Annual Sales)")
scatter.1
```


This first graph uses the variables' original metrics and is almost completely uninterpretable.  There's a clump of data points in the lower left-hand corner of the plot, and then a few outliers. This is why we log-transformed the input variables.


```{r Vizualize Data log Transformed, warning=FALSE, message=FALSE, echo = FALSE}
# Log-transformed
scatter.2 <- ggplot(customers, aes(x = frequency.log, y = monetary.log))
scatter.2 <- scatter.2 + geom_point(aes(colour = recency.log, shape = pareto))
scatter.2 <- scatter.2 + scale_shape_manual(name = "80/20 Designation", values=c(17, 16))
scatter.2 <- scatter.2 + scale_colour_gradient(name="Log-transformed Recency")
scatter.2 <- scatter.2 + xlab("Log-transformed Frequency")
scatter.2 <- scatter.2 + ylab("Log-transformed Monetary Value of Customer")
scatter.2
```

Now we can see a scattering of high-value, high-frequency customers in the top, right-hand corner of the graph. These data points are dark, indicating that they've purchased something recently. In the bottom, left-hand corner of the plot, we can see a couple of low-value, low frequency customers who haven't purchased anything recently, with a range of values in between.


### Handling outliers

One question we might have about those dots in the bottom, left-hand corner is how many customers they represent. The following code investigates them a little more thoroughly.
```{r Handling OUtliers, warning=FALSE, message=FALSE, echo = FALSE}



# How many customers are represented by the two data points in the lower left-hand corner of the plot? 18
delete <- subset(customers, monetary.log < 0)
no.value.custs <- unique(delete$CustomerID)
delete2 <- subset(data, CustomerID %in% no.value.custs)
delete2 <- delete2[order(delete2$CustomerID, delete2$InvoiceDate),]
remove(delete, delete2, no.value.custs)
```

The no-value customers are all customers who returned everything they bought. k-means clustering is sensitive to outliers. Often, outliers will huddle up clustered together in a seld contained grroup. For our experiment, these outliers are deliberately included.


```{r Scaled Variables, warning=FALSE, message=FALSE, echo = FALSE}


# Scaled variables
scatter.3 <- ggplot(customers, aes(x = frequency.z, y = monetary.z))
scatter.3 <- scatter.3 + geom_point(aes(colour = recency.z, shape = pareto))
scatter.3 <- scatter.3 + scale_shape_manual(name = "80/20 Designation", values=c(17, 16))
scatter.3 <- scatter.3 + scale_colour_gradient(name="Z-scored Recency")
scatter.3 <- scatter.3 + xlab("Z-scored Frequency")
scatter.3 <- scatter.3 + ylab("Z-scored Monetary Value of Customer")
scatter.3

remove(scatter.1, scatter.2, scatter.3)


```


## Determine number of clusters / run k-means


```{r Determine number of clusters, warning=FALSE, message=FALSE, echo = FALSE}
preprocessed <- customers[,9:11]
j <- 10 # specify the maximum number of clusters you want to try out

models <- data.frame(k=integer(),
                     tot.withinss=numeric(),
                     betweenss=numeric(),
                     totss=numeric(),
                     rsquared=numeric())

for (k in 1:j ) {
    
    print(k)
    
    # Run kmeans
    # nstart = number of initial configurations; the best one is used
    # $iter will return the iteration used for the final model
    output <- kmeans(preprocessed, centers = k, nstart = 20)
    
    # Add cluster membership to customers dataset
    var.name <- paste("cluster", k, sep="_")
    customers[,(var.name)] <- output$cluster
    customers[,(var.name)] <- factor(customers[,(var.name)], levels = c(1:k))
    
    # Graph clusters
    cluster_graph <- ggplot(customers, aes(x = frequency.log, y = monetary.log))
    cluster_graph <- cluster_graph + geom_point(aes(colour = customers[,(var.name)]))
    colors <- c('red','orange','green3','deepskyblue','blue','darkorchid4','violet','pink1','tan3','black')
    cluster_graph <- cluster_graph + scale_colour_manual(name = "Cluster Group", values=colors)
    cluster_graph <- cluster_graph + xlab("Log-transformed Frequency")
    cluster_graph <- cluster_graph + ylab("Log-transformed Monetary Value of Customer")
    title <- paste("k-means Solution with", k, sep=" ")
    title <- paste(title, "Clusters", sep=" ")
    cluster_graph <- cluster_graph + ggtitle(title)
    print(cluster_graph)
    
    # Cluster centers in original metrics
    library(plyr)
    print(title)
    cluster_centers <- ddply(customers, .(customers[,(var.name)]), summarize,  
                             monetary=round(median(monetary),2),  # use median b/c this is the raw, heavily-skewed data
                             frequency=round(median(frequency),1), 
                             recency=round(median(recency), 0))
    names(cluster_centers)[names(cluster_centers)=="customers[, (var.name)]"] <- "Cluster"
    print(cluster_centers)
    cat("\n")
    cat("\n")
    
    # Collect model information
    models[k,("k")] <- k
    models[k,("tot.withinss")] <- output$tot.withinss # the sum of all within sum of squares
    models[k,("betweenss")] <- output$betweenss
    models[k,("totss")] <- output$totss # betweenss + tot.withinss
    models[k,("rsquared")] <- round(output$betweenss/output$totss, 3) # percentage of variance explained by cluster membership
    assign("models", models, envir = .GlobalEnv) 
    
    remove(output, var.name, cluster_graph, cluster_centers, title, colors)
    
}

remove(k)
```



A 2-cluster solution produces one group of high-value (median = $1,797.78), high-frequency (median = 5 purchases) customers who have purchased recently (median = 17 days since their most recent purchase), and one group of lower value (median = $327.50), low frequency (median = 1 purchase) customers for whom it's been a median of 96 days since their last purchase. Although these two clusters are clear and interpretable, this may be simplifying customer behavior too much.

The 5-cluster solution gives us: a high-value, high-frequency, recent purchase group (cluster 5), a medium-value, medium-frequency, relatively-recent purchase group (cluster 2), two clusters of low-value, low-frequency customers broken down by whether their last purchase was recent or much earlier in the year (clusters 3 and 1, respectively), and lastly a no-value cluster whose median value to the business is $0.00 (cluster 4).

As we move beyond 5 clusters, the graphs become increasingly hard to interpret visually, and the cluster centers start to make distinctions that may not be that helpful (e.g., low-value-with-1-purchase vs. low-value-with-2-purchases customers).




### Skree Plot 

```{r Elbow Skree Plot, warning=FALSE, message=FALSE, echo = FALSE}
library(ggplot2)
library(scales)

# Graph variance explained by number of clusters
r2_graph <- ggplot(models, aes(x = k, y = rsquared))
r2_graph <- r2_graph + geom_point() + geom_line()
r2_graph <- r2_graph + scale_y_continuous(labels = scales::percent)
r2_graph <- r2_graph + scale_x_continuous(breaks = 1:j)
r2_graph <- r2_graph + xlab("k (Number of Clusters)")
r2_graph <- r2_graph + ylab("Variance Explained")
r2_graph

# Graph within sums of squares by number of clusters
# Look for a "bend" in the graph, as with a scree plot
ss_graph <- ggplot(models, aes(x = k, y = tot.withinss))
ss_graph <- ss_graph + geom_point() + geom_line()
ss_graph <- ss_graph + scale_x_continuous(breaks = 1:j)
ss_graph <- ss_graph + scale_y_continuous(labels = scales::comma)
ss_graph <- ss_graph + xlab("k (Number of Clusters)")
ss_graph <- ss_graph + ylab("Total Within SS")
ss_graph

remove(j, r2_graph, ss_graph)
```
Both graphs look to have elbows at around 2 clusters, but a 2-cluster solution explains only 49% of the variance and, once again, a 2-cluster solution may be too much of a simplification to really help the business with targeted marketing. The 5-cluster solution explains ~73% of the variance, but there are no clear elbows in the graph at this point.

###  NbClust Auto Recommendations and Optimizations

Lastly, we can use NbClust package that will look at multitude of different fit indices and, using majority rule, suggest the number of clusters that most indices recommend. 

For computational stress, the section is commented out.


```{r NbClust, warning=FALSE, message=FALSE, echo = FALSE}

# library(NbClust)
# set.seed(1)
# nc <- NbClust(preprocessed, min.nc=2, max.nc=7, method="kmeans")
# table(nc$Best.n[1,])
# 
# nc$All.index # estimates for each number of clusters on 26 different metrics of model fit
# 
# barplot(table(nc$Best.n[1,]), 
#         xlab="Number of Clusters", ylab="Number of Criteria",
#         main="Number of Clusters Chosen by Criteria")
# 
# remove(preprocessed)
```

The greatest number of indices recommend the 2-cluster solution. 



## Three-Dimensional Representation of Clusters


```{r Plot clusters in 3D, warning=FALSE, message=FALSE, echo = FALSE}
colors <- c('red','orange','green3','deepskyblue','blue','darkorchid4','violet','pink1','tan3','black')

library(car)
library(rgl)
dev.off()
scatter3d(x = customers$frequency.log, 
          y = customers$monetary.log,
          z = customers$recency.log, 
          groups = customers$cluster_5,
          xlab = "Frequency (Log-transformed)", 
          ylab = "Monetary Value (log-transformed)",
          zlab = "Recency (Log-transformed)",
          surface.col = colors,
          axis.scales = FALSE,
          surface = TRUE, # produces the horizonal planes through the graph at each level of monetary value
          fit = "smooth",
          #     ellipsoid = TRUE, # to graph ellipses uses this command and set "surface = " to FALSE
          grid = TRUE,
          axis.col = c("black", "black", "black"))

remove(colors)
```

PS: 3 Cluster Model

![3 Cluster Model](https://github.com/prateekchatterji/rstudio-git-test/raw/master/3ClusterModelCapture.PNG)